# Interpretable_Optimization
Interpretable_Optimization

The main objective of this project will be to exploit the untapped synergies between Optimization and Machine Learning
and create models of what we will call Interpretable Optimization, which has a key potential to improve decision
making within institutions and the use of models in policy making.
Interpretable Optimization gets its inspiration from the emerging field of Interpretable Machine Learning. Interpretable
Machine Learning stems from the need of explaining decisions when stakes are high and issues such as overfitting or
machine bias are especially problematic. Interpretable Machine Learning rejects black boxes and instead relies on simple
models that can be easily understood to support decisions.
Interpretable Optimization will apply this idea to Mathematical Optimization and develop methods to explain the
decisions taken with the support of an Optimization model. The key idea is to extract expert knowledge from optimization
models using Interpretable Machine Learning. This will allow creating more useful models and more grounded decisions.
This will be articulated along two main axes:
- 1: Rationale. explain the optimal decision in terms of the features of the problem automatically. This also implies
identifying the most important elements of the problem.
- 2: Panorama: discover quasi-optimal decision alternatives (which I define as decisions that are not the optimal one
but sufficiently close according to the criterion of the decision maker) and analyse what they have in common.